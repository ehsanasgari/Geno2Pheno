{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "import math\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_selection import SelectKBest, SelectFdr\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "\n",
    "class Chi2Anlaysis(object):\n",
    "    # X^2 is statistically significant at the p-value level\n",
    "    def __init__(self,X, Y, feature_names):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.feature_names=feature_names\n",
    "        \n",
    "    def extract_features_kbest(self, file_name, N):\n",
    "        \n",
    "        selector = SelectKBest(chi2,k='all')\n",
    "        selector.fit_transform(self.X, self.Y )\n",
    "        \n",
    "        scores = {self.feature_names[i]: (s,selector.pvalues_[i]) for i, s in enumerate(list(selector.scores_)) if not math.isnan(s) }\n",
    "        scores = sorted(scores.items(), key=operator.itemgetter([1][0]),reverse=True)[0:N]\n",
    "        f = codecs.open(file_name,'w')\n",
    "        f.write('\\t'.join(['feature', 'score', 'p-value', '#I-out-of-'+str(np.sum(self.Y)), '#O-out-of-'+str(len(self.Y)-np.sum(self.Y))])+'\\n')\n",
    "        for w, score in scores:\n",
    "            feature_array=self.X[:,self.feature_names.index(w)]\n",
    "            pos=[feature_array[idx] for idx, x in enumerate(self.Y) if x==1]\n",
    "            neg=[feature_array[idx] for idx, x in enumerate(self.Y) if x==0]\n",
    "            f.write('\\t'.join([str(w), str(score[0]), str(score[1]), str(np.sum(pos)), str(np.sum(neg))])+'\\n')        \n",
    "        f.close()\n",
    "    \n",
    "    def extract_features_fdr(self, file_name, N):\n",
    "        #https://brainder.org/2011/09/05/fdr-corrected-fdr-adjusted-p-values/\n",
    "        #Filter: Select the p-values for an estimated false discovery rate\n",
    "        #This uses the Benjamini-Hochberg procedure. alpha is an upper bound on the expected false discovery rate.\n",
    "        selector = SelectFdr(chi2,alpha=1e-2)\n",
    "        selector.fit_transform(self.X, self.Y )\n",
    "        scores = {self.feature_names[i]: (s,selector.pvalues_[i]) for i, s in enumerate(list(selector.scores_)) if not math.isnan(s)}\n",
    "        scores = sorted(scores.items(), key=operator.itemgetter([1][0]),reverse=True)[0:N]\n",
    "        f = codecs.open(file_name,'w')\n",
    "        f.write('\\t'.join(['feature', 'score', 'p-value', '#I-out-of-'+str(np.sum(self.Y)), '#O-out-of-'+str(len(self.Y)-np.sum(self.Y))])+'\\n')\n",
    "        for w, score in scores:\n",
    "            feature_array=self.X[:,self.feature_names.index(w)]\n",
    "            pos=[feature_array[idx] for idx, x in enumerate(self.Y) if x==1]\n",
    "            neg=[feature_array[idx] for idx, x in enumerate(self.Y) if x==0]\n",
    "            f.write('\\t'.join([str(w), str(score[0]), str(score[1]), str(np.sum(pos)), str(np.sum(neg))])+'\\n')        \n",
    "        f.close()    \n",
    "class TextFeature(object):\n",
    "    def __init__(self, corpus, analyzer='word', ngram=(1,1)):\n",
    "        tfm = TfidfVectorizer(use_idf=False, analyzer=analyzer, ngram_range=ngram, norm=None, stop_words=[], lowercase=False)\n",
    "        self.tf_vec = tfm.fit_transform(corpus).toarray()\n",
    "        self.feature_names = tfm.get_feature_names()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "import codecs\n",
    "import itertools\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "\n",
    "class ClassifierTuning(object):\n",
    "    def __init__(self, X, Y, clf_0, parameters):\n",
    "        self.cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "        self.clf = GridSearchCV(estimator=clf_0, param_grid=parameters, cv=self.cv, n_jobs=30, scoring='f1')\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def find_best(self, filename):\n",
    "        self.clf.fit(self.X, self.Y)\n",
    "        with open( filename + '.pickle', 'wb') as f:\n",
    "            pickle.dump(self.clf, f)\n",
    "        return self.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#RF = pickle.load( open(\"tuned_params/random_forst_Tobramycin.pickle\", \"rb\" ) )\n",
    "#RF.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.clf_random_forest=RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "            max_depth=None, max_features='auto', min_samples_leaf=1, n_estimators=500, n_jobs=30)\n",
    "\n",
    "    def evaluate_through_cv(self,folds=10, test_portion=0.1, multiclass=True, extensive_measures=False):\n",
    "        cv = ShuffleSplit(n_splits=folds, test_size=test_portion, random_state=1)\n",
    "        if not multiclass:\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='precision')\n",
    "            self.precision=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='recall')\n",
    "            self.recall=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='f1')\n",
    "            self.f1=(scores.mean(), scores.std())\n",
    "        if multiclass:\n",
    "            if extensive_measures:\n",
    "                scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='precision_macro')\n",
    "                self.precision_macro=(scores.mean(), scores.std())\n",
    "                scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='recall_macro')\n",
    "                self.recall_macro=(scores.mean(), scores.std())\n",
    "                scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='precision_weighted')\n",
    "                self.precision_weighted=(scores.mean(), scores.std())\n",
    "                scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='recall_weighted')\n",
    "                self.recall_weighted=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='f1_weighted')\n",
    "            self.f1_weighted=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='f1_macro')\n",
    "            self.f1_macro=(scores.mean(), scores.std())\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        y_pred=self.clf_random_forest.fit(X_train, y_train).predict(X_test)\n",
    "        self.showing_labels=list(set(Y))\n",
    "        self.showing_labels.sort()\n",
    "        self.confusion=confusion_matrix(y_test, y_pred,labels=self.showing_labels)\n",
    "        \n",
    "    \n",
    "    def get_important_features(self,file_name, N, labels):\n",
    "        self.clf_random_forest.fit(self.X, self.Y)\n",
    "        std = np.std([tree.feature_importances_ for tree in self.clf_random_forest.estimators_],axis=0)\n",
    "\n",
    "        scores = {feature_names[i]: (s,std[i]) for i, s in enumerate(list(self.clf_random_forest.feature_importances_)) if not math.isnan(s) }\n",
    "        scores = sorted(scores.items(), key=operator.itemgetter([1][0]),reverse=True)[0:N]\n",
    "        f = codecs.open(file_name,'w')\n",
    "        f.write('\\t'.join(['feature', 'score', 'std', '#I-out-of-'+str(np.sum(self.Y)), '#O-out-of-'+str(len(self.Y)-np.sum(self.Y))])+'\\n')\n",
    "        for w, score in scores:\n",
    "            feature_array=self.X[:,feature_names.index(w)]\n",
    "            pos=[feature_array[idx] for idx, x in enumerate(self.Y) if x==1]\n",
    "            neg=[feature_array[idx] for idx, x in enumerate(self.Y) if x==0]\n",
    "            f.write('\\t'.join([str(w), str(score[0]), str(score[1]), str(np.sum(pos)), str(np.sum(neg))])+'\\n')        \n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "class ABRUtility(object):\n",
    "    '''\n",
    "    This class is written to read/load/save\n",
    "    data for AMR prediction of Pseudomonas Aeruginosa\n",
    "    '''\n",
    "    def __init__(self, data_dir, labels_addr, genexp_data_addr, snps_data_addr, gen_pres_abs_data_addr):\n",
    "        print ('Data access created..')\n",
    "        self.data_dir=data_dir\n",
    "        self.labels_addr=labels_addr\n",
    "        self.genexp_data_addr=genexp_data_addr\n",
    "        self.snps_data_addr=snps_data_addr\n",
    "        self.gen_pres_abs_data_addr=gen_pres_abs_data_addr\n",
    "\n",
    "    def produce_feature_matrix(self, feature_file_address):\n",
    "        '''\n",
    "        :param feature_file_address:\n",
    "        :return:\n",
    "        '''\n",
    "        rows=[l.strip() for l in codecs.open(feature_file_address,'r','utf-8').readlines()]\n",
    "        mapping_isolate2feature={str(entry.split('\\t')[0]):[float(str(x)) for x in entry.split('\\t')[1::]] for entry in rows[1::]}\n",
    "        features=rows[0].rstrip().split('\\t')\n",
    "        return mapping_isolate2feature, features\n",
    "\n",
    "    def produce_label_str_vec(self, label_file_address, mapping={'0':0,'0.0':0,'1':1,'1.0':1}):\n",
    "        '''\n",
    "            This function produces instances like: ZG02420619 ['1.0', '0.0', '0.0', '', '1.0']\n",
    "        '''\n",
    "        rows=[l.replace('\\n','') for l in codecs.open(label_file_address,'r','utf-8').readlines()]\n",
    "        isolate2label_vec_mapping={str(entry.split('\\t')[0]):[str(x) for idx,x in enumerate(entry.split('\\t')[1::])] for entry in rows[1::]}\n",
    "        labels=rows[0].rstrip().split('\\t')[1::]\n",
    "\n",
    "        # init\n",
    "        drug2labeledisolates_mapping=dict()\n",
    "        for label in labels:\n",
    "            drug2labeledisolates_mapping[label]=[]\n",
    "\n",
    "        # only consider non-empty values\n",
    "        for isolate,resist_vec in isolate2label_vec_mapping.items():\n",
    "            for idx, val in enumerate(resist_vec):\n",
    "                if val in mapping:\n",
    "                    drug2labeledisolates_mapping[labels[idx]].append((isolate,mapping[val]))\n",
    "        # generate dict of labels for each class\n",
    "        for label in labels:\n",
    "            drug2labeledisolates_mapping[label]=dict(drug2labeledisolates_mapping[label])\n",
    "        return isolate2label_vec_mapping, labels, drug2labeledisolates_mapping\n",
    "\n",
    "\n",
    "    def common_isolates(self, list_of_list_of_isolates):\n",
    "        '''\n",
    "        :param list_of_list_of_isolates:\n",
    "        :return:\n",
    "        '''\n",
    "        common_islt=set(list_of_list_of_isolates[0])\n",
    "        for next_list in list_of_list_of_isolates[1::]:\n",
    "            common_islt=common_islt.intersection(next_list)\n",
    "        common_islt=list(common_islt)\n",
    "        common_islt.sort()\n",
    "        return common_islt\n",
    "\n",
    "    def make_matrix(self, mapping, isolate_lsit):\n",
    "        '''\n",
    "        :param mapping:\n",
    "        :param isolate_lsit:\n",
    "        :return:\n",
    "        '''\n",
    "        return np.array([mapping[x] for x in isolate_lsit])\n",
    "\n",
    "    def makeXY_prediction(self, X, label_dict, common_islt):\n",
    "        '''\n",
    "        :param X:\n",
    "        :param label_dict:\n",
    "        :param common_islt:\n",
    "        :return:\n",
    "        '''\n",
    "        rows=[]\n",
    "        labels=[]\n",
    "        for isolate, label in label_dict.items():\n",
    "            if isolate in common_islt:\n",
    "                rows.append(common_islt.index(isolate))\n",
    "                labels.append(label)\n",
    "        return X[rows,:],labels\n",
    "\n",
    "    def make_balanced_dataset(self, X, Y, coeff=1):\n",
    "        '''\n",
    "        :param X:\n",
    "        :param Y:\n",
    "        :param coeff:\n",
    "        :return:\n",
    "        '''\n",
    "        all_idx=list(range(len(Y)))\n",
    "        select_idx=[idx for idx,v in enumerate(Y) if v ==1]\n",
    "        large_list=list(set(all_idx) - set(select_idx))\n",
    "        random.shuffle(large_list)\n",
    "        select_rand=large_list[0:coeff*len(select_idx)]\n",
    "        rows=select_idx+select_rand\n",
    "        return X[rows,:], [Y[x] for x in rows]\n",
    "\n",
    "    def load_SNPs_PAs_features(self, mapping={'0':0,'0.0':0,'1':1,'1.0':1}):\n",
    "        '''\n",
    "        :param data_dir:\n",
    "        :param labels_addr:\n",
    "        :param snps_data_addr:\n",
    "        :param gen_pres_abs_data_addr:\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # feature reading\n",
    "        snps_isolate2feature_mapping, snps_features=self.produce_feature_matrix(self.data_dir+self.snps_data_addr)\n",
    "        genpa_isolate2feature_mapping, genpa_features=self.produce_feature_matrix(self.data_dir+self.gen_pres_abs_data_addr)\n",
    "        \n",
    "        # find isolates with all features\n",
    "        self.common_islt=self.common_isolates([snps_isolate2feature_mapping.keys(),genpa_isolate2feature_mapping.keys()])\n",
    "        X_snp=self.make_matrix(snps_isolate2feature_mapping, self.common_islt)\n",
    "        X_gene_pna=self.make_matrix(genpa_isolate2feature_mapping, self.common_islt)\n",
    "\n",
    "        # concatinate the features\n",
    "        self.X=np.concatenate((X_snp,X_gene_pna), axis=1)\n",
    "                          \n",
    "        # label of features\n",
    "        self.feature_labels=['snp_'+x for x in snps_features]+ ['gene_pa_'+x for x in genpa_features]\n",
    "        \n",
    "        # label reading\n",
    "        self.isolate2label_vec_mapping, self.drug_names, self.drug2labeledisolates_mapping=self.produce_label_str_vec(self.data_dir+self.labels_addr,mapping)\n",
    "        \n",
    "    def load_all_features(self,  mapping={'0':0,'0.0':0,'1':1,'1.0':1}):\n",
    "        '''\n",
    "        :param data_dir:\n",
    "        :param labels_addr:\n",
    "        :param genexp_data_addr:\n",
    "        :param snps_data_addr:\n",
    "        :param gen_pres_abs_data_addr:\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # feature reading\n",
    "        genexp_isolate2feature_mapping, genexp_features=self.produce_feature_matrix(self.data_dir+self.genexp_data_addr)\n",
    "        snps_isolate2feature_mapping, snps_features=self.produce_feature_matrix(self.data_dir+self.snps_data_addr)\n",
    "        genpa_isolate2feature_mapping, genpa_features=self.produce_feature_matrix(self.data_dir+self.gen_pres_abs_data_addr)\n",
    "        \n",
    "        # find isolates with all features\n",
    "        self.common_islt=self.common_isolates([genexp_isolate2feature_mapping.keys(),snps_isolate2feature_mapping.keys(),genpa_isolate2feature_mapping.keys()])\n",
    "        X_gene_exp=self.make_matrix(genexp_isolate2feature_mapping, self.common_islt)\n",
    "        X_snp=self.make_matrix(snps_isolate2feature_mapping, self.common_islt)\n",
    "        X_gene_pna=self.make_matrix(genpa_isolate2feature_mapping, self.common_islt)\n",
    "\n",
    "        # concatinate the features\n",
    "        self.X=np.concatenate((X_gene_exp,X_snp,X_gene_pna), axis=1)\n",
    "                          \n",
    "        # label of features\n",
    "        self.feature_labels=['genexp_'+x for x in genexp_features]+['snp_'+x for x in snps_features]+ ['gene_pa_'+x for x in genpa_features]\n",
    "        \n",
    "        # label reading\n",
    "        self.isolate2label_vec_mapping, self.drug_names, self.drug2labeledisolates_mapping=self.produce_label_str_vec(self.data_dir+self.labels_addr,mapping)\n",
    "\n",
    "    def update_mapping(self, mapping):\n",
    "        self.isolate2label_vec_mapping, self.drug_names, self.drug2labeledisolates_mapping=self.produce_label_str_vec(self.data_dir+self.labels_addr,mapping)\n",
    "        \n",
    "    def get_multilabeltoword(self):\n",
    "        mapping={'':'I','0':'D','0.0':'D','1':'R','1.0':'R'}\n",
    "        return {k:''.join([mapping[x] for x in list(v)]) for k,v in self.isolate2label_vec_mapping.items()}\n",
    "    \n",
    "    def drug_similarity(self, fileaddress):\n",
    "        multilabels=list(DA.get_multilabeltoword().values())\n",
    "        N=len(self.drug_names)\n",
    "        MI=np.zeros((N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                MI[i,j]=metrics.mutual_info_score([label[i] for label in list(multilabels)],[label[j] for label in list(multilabels)])\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticklabels(['']+self.drug_names,rotation=45)\n",
    "        ax.set_yticklabels(['']+self.drug_names)\n",
    "        plt.imshow(MI, cmap='binary' )\n",
    "        plt.savefig(fileaddress)\n",
    "        return MI\n",
    "    \n",
    "    def resistance_frequency_analysis(self):\n",
    "        multilabels=list(self.get_multilabeltoword().values())\n",
    "        return FreqDist(multilabels)\n",
    "    \n",
    "    def return_most_k_frequent_classes(self, n):\n",
    "        most_common_labels=[x for x,y in self.resistance_frequency_analysis().most_common(n)]\n",
    "        joint_labels=self.get_multilabeltoword()\n",
    "        selected_isolates=[x for x in self.common_islt ]#if joint_labels[x] in most_common_labels]\n",
    "        X,Y=self.makeXY_prediction(self.X, self.get_multilabeltoword(), selected_isolates)\n",
    "        return X, Y, selected_isolates\n",
    "\n",
    "    def return_ova_labeling(self):\n",
    "        joint_labels=self.get_multilabeltoword()\n",
    "        all_labels=list(set(joint_labels.values()))\n",
    "        X,Y=self.makeXY_prediction(self.X, self.get_multilabeltoword(), self.common_islt)\n",
    "        labeling_schemes=dict()\n",
    "        for label in all_labels:\n",
    "            labeling_schemes[label]=[1 if y==label else 0 for y in Y]\n",
    "        return X, labeling_schemes\n",
    "    \n",
    "    def classifier_tuning(self, name, classifier, parameters):\n",
    "        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "        params=dict()\n",
    "        for drug in self.drug_names:\n",
    "            print ('parameter tuning for drug ',drug)\n",
    "            Xpart, Y=self.makeXY_prediction(self.X, self.drug2labeledisolates_mapping[drug], self.common_islt)\n",
    "            CT=ClassifierTuning(Xpart, Y, classifier, parameters)\n",
    "            CT.find_best(name+'_'+drug)\n",
    "            params[drug]=CT.clf\n",
    "        return params\n",
    "    \n",
    "    def classifier_testing_RF(self, parameters_file_prefix):\n",
    "        results=dict()\n",
    "        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "        for drug in self.drug_names:\n",
    "            print (drug)\n",
    "            results[drug]=[]\n",
    "            Xpart, Y=self.makeXY_prediction(self.X, self.drug2labeledisolates_mapping[drug], self.common_islt)\n",
    "            RF = pickle.load(open('tuned_params/'+parameters_file_prefix+\"_\"+drug+\".pickle\", \"rb\"))\n",
    "            parameters=RF.best_params_\n",
    "            clf_random_forest=RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "            max_depth=None, max_features=parameters['max_features'], min_samples_split=parameters['min_samples_split'] , min_samples_leaf=1, n_estimators=parameters['n_estimators'], n_jobs=30)\n",
    "            scores = cross_val_score(clf_random_forest, Xpart, Y, cv=cv, scoring='precision')\n",
    "            print(\"precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            results[drug].append(('RF','precision',scores.mean(), scores.std()))\n",
    "\n",
    "            scores = cross_val_score(clf_random_forest,Xpart, Y, cv=cv, scoring='recall')\n",
    "            print(\"recall: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            results[drug].append(('RF','recall',scores.mean(), scores.std()))\n",
    "\n",
    "            scores = cross_val_score(clf_random_forest, Xpart, Y, cv=cv, scoring='f1')\n",
    "            print(\"f1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            results[drug].append(('RF','f1',scores.mean(), scores.std()))\n",
    "\n",
    "            #scores = cross_val_score(clf_random_forest, Xpart, Y, cv=cv, scoring='roc_auc')\n",
    "            #print(\"roc_auc: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            #results[drug].append(('RF','roc_auc',scores.mean(), scores.std()))\n",
    "    def extract_joint_relevant_features(self):\n",
    "        X,labeling_scheme=self.return_ova_labeling()\n",
    "        feature_names = self.feature_labels\n",
    "\n",
    "        frequents=self.resistance_frequency_analysis().most_common(100)\n",
    "\n",
    "        for rank, (l,freq) in enumerate(frequents):\n",
    "            print (l)\n",
    "            L=labeling_scheme[l]\n",
    "\n",
    "            CA=Chi2Anlaysis(X, L, feature_names)\n",
    "            CA.extract_features_fdr('extracted_features/joint_drug/'+str(rank+1)+'_'+l+'_'+str(freq)+'_chi2_fdr_corrected.txt', 100)\n",
    "            CA.extract_features_kbest('extracted_features/joint_drug/'+str(rank+1)+'_'+l+'_'+str(freq)+'_chi2_kbest.txt', 100)\n",
    "\n",
    "    def extract_randomforest_features(self):\n",
    "        X,labeling_scheme=self.return_ova_labeling()\n",
    "        feature_names = self.feature_labels\n",
    "\n",
    "        frequents=self.resistance_frequency_analysis().most_common(100)\n",
    "\n",
    "        for rank, (l,freq) in enumerate(frequents):\n",
    "            print (l)\n",
    "            L=labeling_scheme[l]\n",
    "            RF=RandomForest(X, L)\n",
    "            RF.get_important_features('extracted_features/joint_drug/RF_'+str(rank+1)+'_'+l+'_'+str(freq)+'.txt', 100, feature_names)\n",
    "            \n",
    "    def extract_drug_specific_features(self):\n",
    "        feature_names = self.feature_labels\n",
    "\n",
    "        for drug in self.drug_names:\n",
    "            print (drug)\n",
    "            Xpart, Y=self.makeXY_prediction(self.X, self.drug2labeledisolates_mapping[drug], self.common_islt)\n",
    "            CA=Chi2Anlaysis(Xpart, Y, feature_names)\n",
    "            CA.extract_features_fdr('extracted_features/separate_drug/IvsAll_'+drug+'_chi2_fdr_corrected.txt', 100)\n",
    "            CA.extract_features_kbest('extracted_features/separate_drug/IvsAll_'+drug+'_chi2_kbest.txt', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data access created..\n"
     ]
    }
   ],
   "source": [
    "data_dir='/mounts/data/proj/asgari/github_data/data/pseudomonas/data/'\n",
    "labels_addr='MIC/v2/mic_bin_without_intermediate.txt'\n",
    "genexp_data_addr='new/gene_expression/rpg_log_transformed_426.txt'\n",
    "snps_data_addr='snp/v2/non-syn_SNPs_bin.txt'\n",
    "gen_pres_abs_data_addr='new/annot.txt'\n",
    "\n",
    "DA=ABRUtility(data_dir, labels_addr, genexp_data_addr, snps_data_addr, gen_pres_abs_data_addr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DA.load_SNPs_PAs_features(mapping={'1.0':1,'1':1,'0.0':0,'0':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRDRR\n",
      "DDDDD\n",
      "RDDRR\n",
      "RRIRR\n",
      "DDDDR\n",
      "RDDDR\n",
      "DDDRR\n",
      "RRDIR\n",
      "IDDRR\n",
      "RDDIR\n",
      "RDIRR\n",
      "RDDDD\n",
      "RRDDR\n",
      "DDDRD\n",
      "RRDRD\n",
      "DDDIR\n",
      "IDDDD\n",
      "RRRRR\n",
      "DDDDI\n",
      "DDDID\n",
      "DDIDD\n",
      "IDDDI\n",
      "RRIIR\n",
      "RDIIR\n",
      "IDDIR\n",
      "DRDRR\n",
      "RIDRR\n",
      "RDIDR\n",
      "RDDII\n",
      "DRDDD\n",
      "IDIRR\n",
      "RRDRI\n",
      "RIIRR\n",
      "RDRRR\n",
      "DIDRR\n",
      "DDIRR\n",
      "DDIRI\n",
      "IDDDR\n",
      "RIDDR\n",
      "IDDID\n",
      "RRIRD\n",
      "RDRDD\n",
      "RDDRI\n",
      "RDIRI\n",
      "RDIID\n",
      "DDIRD\n",
      "IIDRR\n",
      "DDIDI\n",
      "RIRRR\n",
      "RDRIR\n",
      "RDDDI\n",
      "DDDII\n",
      "RDRDR\n",
      "RDDID\n",
      "RRDDI\n",
      "IDIDR\n",
      "RRRRI\n",
      "DRRID\n",
      "RIDRD\n",
      "IRDDR\n",
      "IIDIR\n",
      "IDDRD\n",
      "RRIDI\n",
      "RRDII\n",
      "RRRDD\n",
      "DRRRR\n",
      "DIDIR\n",
      "IDIRI\n",
      "RRDID\n",
      "IRDRD\n",
      "DIIII\n",
      "DIRRR\n",
      "RDDRD\n",
      "DDRDI\n",
      "RIRDD\n",
      "DRDRD\n",
      "IRDIR\n",
      "DRDID\n",
      "RRIDR\n",
      "DDDRI\n",
      "RRDDD\n",
      "DDIID\n",
      "DRDDI\n",
      "IDRRI\n"
     ]
    }
   ],
   "source": [
    "DA.extract_randomforest_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RRDRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/asgari/.local/lib/python3.4/site-packages/sklearn/feature_selection/univariate_selection.py:600: RuntimeWarning: invalid value encountered in less_equal\n",
      "  np.arange(1, n_features + 1)]\n",
      "/mounts/Users/student/asgari/.local/lib/python3.4/site-packages/sklearn/feature_selection/univariate_selection.py:603: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return self.pvalues_ <= selected.max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDDDD\n",
      "RDDRR\n",
      "RRIRR\n",
      "DDDDR\n",
      "RDDDR\n",
      "DDDRR\n",
      "RRDIR\n",
      "IDDRR\n",
      "RDDIR\n",
      "RDIRR\n",
      "RDDDD\n",
      "RRDDR\n",
      "DDDRD\n",
      "RRDRD\n",
      "DDDIR\n",
      "IDDDD\n",
      "RRRRR\n",
      "DDDDI\n",
      "DDDID\n",
      "DDIDD\n",
      "IDDDI\n",
      "RRIIR\n",
      "RDIIR\n",
      "IDDIR\n",
      "DRDRR\n",
      "RIDRR\n",
      "RDIDR\n",
      "RDDII\n",
      "DRDDD\n",
      "IDIRR\n",
      "RRDRI\n",
      "RIIRR\n",
      "RDRRR\n",
      "DIDRR\n",
      "DDIRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/asgari/.local/lib/python3.4/site-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDIRI\n",
      "IDDDR\n",
      "RIDDR\n",
      "IDDID\n",
      "RRIRD\n",
      "RDRDD\n",
      "RDDRI\n",
      "RDIRI\n",
      "RDIID\n",
      "DDIRD\n",
      "IIDRR\n",
      "DDIDI\n",
      "RIRRR\n",
      "RDRIR\n",
      "RDDDI\n",
      "DDDII\n",
      "RDRDR\n",
      "RDDID\n",
      "RRDDI\n",
      "IDIDR\n",
      "RRRRI\n",
      "DRRID\n",
      "RIDRD\n",
      "IRDDR\n",
      "IIDIR\n",
      "IDDRD\n",
      "RRIDI\n",
      "RRDII\n",
      "RRRDD\n",
      "DRRRR\n",
      "DIDIR\n",
      "IDIRI\n",
      "RRDID\n",
      "IRDRD\n",
      "DIIII\n",
      "DIRRR\n",
      "RDDRD\n",
      "DDRDI\n",
      "RIRDD\n",
      "DRDRD\n",
      "IRDIR\n",
      "DRDID\n",
      "RRIDR\n",
      "DDDRI\n",
      "RRDDD\n",
      "DDIID\n",
      "DRDDI\n",
      "IDRRI\n"
     ]
    }
   ],
   "source": [
    "DA.extract_joint_relevant_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DA.update_mapping(mapping={'':1,'1.0':0,'1':0,'0.0':0,'0':0})\n",
    "#mapping={'':0,'0.0':1,'0':1,'1.0':0,'1':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, Y, selected_isolates=DA.return_most_k_frequent_classes(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RRDRR', 46), ('DDDDD', 35), ('DDDDR', 29), ('RDDRR', 25), ('RRIRR', 16)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist(Y).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_random_forest=RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "            max_depth=None, max_features='auto', min_samples_leaf=1, n_estimators=500, n_jobs=30)\n",
    "\n",
    "param_grid = {\"n_estimators\": [100,200, 500, 1000],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_split':[2,5,10]}\n",
    "\n",
    "#RF_params=DA.classifier_tuning('random_forst',clf_random_forest,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_svm=svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "param_grid = {'kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "             'C':list(range())}\n",
    "\n",
    "RF_params=DA.classifier_tuning('svm',clf_random_forest,param_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
