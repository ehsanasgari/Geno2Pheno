{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "import codecs\n",
    "import itertools\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "\n",
    "class ClassifierTuning(object):\n",
    "    def __init__(self, X, Y, clf_0, parameters):\n",
    "        self.cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "        self.clf = GridSearchCV(estimator=clf_0, param_grid=parameters, cv=self.cv, n_jobs=30, scoring='f1')\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def find_best(self, filename):\n",
    "        self.clf.fit(self.X, self.Y)\n",
    "        with open( filename + '.pickle', 'wb') as f:\n",
    "            pickle.dump(self.clf, f)\n",
    "        return self.clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#RF = pickle.load( open(\"tuned_params/random_forst_Tobramycin.pickle\", \"rb\" ) )\n",
    "#RF.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class RandomForest(object):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.clf_random_forest=RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "            max_depth=None, max_features='auto', min_samples_leaf=1, n_estimators=500, n_jobs=30)\n",
    "\n",
    "    def evaluate_through_cv(self,folds=10, test_portion=0.1, multiclass=True):\n",
    "        cv = ShuffleSplit(n_splits=folds, test_size=test_portion, random_state=1)\n",
    "        if not multiclass:\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='precision')\n",
    "            self.precision=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='recall')\n",
    "            self.recall=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='f1')\n",
    "            self.f1=(scores.mean(), scores.std())\n",
    "        if multiclass:\n",
    "            #scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='precision_macro')\n",
    "            #self.precision_macro=(scores.mean(), scores.std())\n",
    "            #scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='recall_macro')\n",
    "            #self.recall_macro=(scores.mean(), scores.std())\n",
    "            #scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='precision_weighted')\n",
    "            #self.precision_weighted=(scores.mean(), scores.std())\n",
    "            #scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='recall_weighted')\n",
    "            #self.recall_weighted=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='f1_weighted')\n",
    "            self.f1_weighted=(scores.mean(), scores.std())\n",
    "            scores = cross_val_score(self.clf_random_forest, X, Y, cv=cv, scoring='f1_macro')\n",
    "            self.f1_macro=(scores.mean(), scores.std())\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        y_pred=self.clf_random_forest.fit(X_train, y_train).predict(X_test)\n",
    "        self.showing_labels=list(set(Y))\n",
    "        self.showing_labels.sort()\n",
    "        self.confusion=confusion_matrix(y_test, y_pred,labels=self.showing_labels)\n",
    "        \n",
    "    \n",
    "    def get_important_features(self, N, labels):\n",
    "        self.clf_random_forest.fit(self.X, self.Y)\n",
    "        importances = self.clf_random_forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in self.clf_random_forest.estimators_],axis=0)\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        # Print the feature ranking\n",
    "        results=[]\n",
    "        print(\"Feature ranking:\")\n",
    "        for f in range(self.X.shape[1]):\n",
    "            if f<N:\n",
    "                print(\"%d. feature %s (%f, %f)\" % (f + 1, labels[indices[f]], importances[indices[f]],std[indices[f]]))\n",
    "            results.append((f + 1, labels[indices[f]], importances[indices[f]],std[indices[f]]))\n",
    "        return results \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Ehsaneddin Asgari\"\n",
    "__copyright__ = \"Copyright 2017, HH-HZI Project\"\n",
    "__license__ = \"GPL\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Ehsaneddin Asgari\"\n",
    "__email__ = \"asgari@berkeley.edu\"\n",
    "\n",
    "import codecs\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "class ABRUtility(object):\n",
    "    '''\n",
    "    This class is written to read/load/save\n",
    "    data for AMR prediction of Pseudomonas Aeruginosa\n",
    "    '''\n",
    "    def __init__(self, data_dir, labels_addr, genexp_data_addr, snps_data_addr, gen_pres_abs_data_addr):\n",
    "        print ('Data access created..')\n",
    "        self.data_dir=data_dir\n",
    "        self.labels_addr=labels_addr\n",
    "        self.genexp_data_addr=genexp_data_addr\n",
    "        self.snps_data_addr=snps_data_addr\n",
    "        self.gen_pres_abs_data_addr=gen_pres_abs_data_addr\n",
    "\n",
    "    def produce_feature_matrix(self, feature_file_address):\n",
    "        '''\n",
    "        :param feature_file_address:\n",
    "        :return:\n",
    "        '''\n",
    "        rows=[l.strip() for l in codecs.open(feature_file_address,'r','utf-8').readlines()]\n",
    "        mapping_isolate2feature={str(entry.split('\\t')[0]):[float(str(x)) for x in entry.split('\\t')[1::]] for entry in rows[1::]}\n",
    "        features=rows[0].rstrip().split('\\t')\n",
    "        return mapping_isolate2feature, features\n",
    "\n",
    "    def produce_label_str_vec(self, label_file_address):\n",
    "        '''\n",
    "            This function produces instances like: ZG02420619 ['1.0', '0.0', '0.0', '', '1.0']\n",
    "        '''\n",
    "        rows=[l.replace('\\n','') for l in codecs.open(label_file_address,'r','utf-8').readlines()]\n",
    "        isolate2label_vec_mapping={str(entry.split('\\t')[0]):[str(x) for idx,x in enumerate(entry.split('\\t')[1::])] for entry in rows[1::]}\n",
    "        labels=rows[0].rstrip().split('\\t')[1::]\n",
    "\n",
    "        # init\n",
    "        drug2labeledisolates_mapping=dict()\n",
    "        for label in labels:\n",
    "            drug2labeledisolates_mapping[label]=[]\n",
    "\n",
    "        # only consider non-empty values\n",
    "        for isolate,resist_vec in isolate2label_vec_mapping.items():\n",
    "            for idx, val in enumerate(resist_vec):\n",
    "                if val in ['0','0.0','1','1.0']:\n",
    "                    drug2labeledisolates_mapping[labels[idx]].append((isolate,int(float(val))))\n",
    "        # generate dict of labels for each class\n",
    "        for label in labels:\n",
    "            drug2labeledisolates_mapping[label]=dict(drug2labeledisolates_mapping[label])\n",
    "        return isolate2label_vec_mapping, labels, drug2labeledisolates_mapping\n",
    "\n",
    "\n",
    "    def common_isolates(self, list_of_list_of_isolates):\n",
    "        '''\n",
    "        :param list_of_list_of_isolates:\n",
    "        :return:\n",
    "        '''\n",
    "        common_islt=set(list_of_list_of_isolates[0])\n",
    "        for next_list in list_of_list_of_isolates[1::]:\n",
    "            common_islt=common_islt.intersection(next_list)\n",
    "        common_islt=list(common_islt)\n",
    "        common_islt.sort()\n",
    "        return common_islt\n",
    "\n",
    "    def make_matrix(self, mapping, isolate_lsit):\n",
    "        '''\n",
    "        :param mapping:\n",
    "        :param isolate_lsit:\n",
    "        :return:\n",
    "        '''\n",
    "        return np.array([mapping[x] for x in isolate_lsit])\n",
    "\n",
    "    def makeXY_prediction(self, X, label_dict, common_islt):\n",
    "        '''\n",
    "        :param X:\n",
    "        :param label_dict:\n",
    "        :param common_islt:\n",
    "        :return:\n",
    "        '''\n",
    "        rows=[]\n",
    "        labels=[]\n",
    "        for isolate, label in label_dict.items():\n",
    "            if isolate in common_islt:\n",
    "                rows.append(common_islt.index(isolate))\n",
    "                labels.append(label)\n",
    "        return X[rows,:],labels\n",
    "\n",
    "    def make_balanced_dataset(self, X, Y, coeff=1):\n",
    "        '''\n",
    "        :param X:\n",
    "        :param Y:\n",
    "        :param coeff:\n",
    "        :return:\n",
    "        '''\n",
    "        all_idx=list(range(len(Y)))\n",
    "        select_idx=[idx for idx,v in enumerate(Y) if v ==1]\n",
    "        large_list=list(set(all_idx) - set(select_idx))\n",
    "        random.shuffle(large_list)\n",
    "        select_rand=large_list[0:coeff*len(select_idx)]\n",
    "        rows=select_idx+select_rand\n",
    "        return X[rows,:], [Y[x] for x in rows]\n",
    "\n",
    "    def load_all_features(self):\n",
    "        '''\n",
    "        :param data_dir:\n",
    "        :param labels_addr:\n",
    "        :param genexp_data_addr:\n",
    "        :param snps_data_addr:\n",
    "        :param gen_pres_abs_data_addr:\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # feature reading\n",
    "        genexp_isolate2feature_mapping, genexp_features=self.produce_feature_matrix(self.data_dir+self.genexp_data_addr)\n",
    "        snps_isolate2feature_mapping, snps_features=self.produce_feature_matrix(self.data_dir+self.snps_data_addr)\n",
    "        genpa_isolate2feature_mapping, genpa_features=self.produce_feature_matrix(self.data_dir+self.gen_pres_abs_data_addr)\n",
    "        \n",
    "        # find isolates with all features\n",
    "        self.common_islt=self.common_isolates([genexp_isolate2feature_mapping.keys(),snps_isolate2feature_mapping.keys(),genpa_isolate2feature_mapping.keys()])\n",
    "        X_gene_exp=self.make_matrix(genexp_isolate2feature_mapping, self.common_islt)\n",
    "        X_snp=self.make_matrix(snps_isolate2feature_mapping, self.common_islt)\n",
    "        X_gene_pna=self.make_matrix(genpa_isolate2feature_mapping, self.common_islt)\n",
    "\n",
    "        # concatinate the features\n",
    "        self.X=np.concatenate((X_gene_exp,X_snp,X_gene_pna), axis=1)\n",
    "                          \n",
    "        # label of features\n",
    "        self.feature_labels=['genexp_'+x for x in genexp_features]+['snp_'+x for x in snps_features]+ ['gene_pa_'+x for x in genpa_features]\n",
    "        \n",
    "        # label reading\n",
    "        self.isolate2label_vec_mapping, self.drug_names, self.drug2labeledisolates_mapping=self.produce_label_str_vec(self.data_dir+self.labels_addr)\n",
    "\n",
    "    def get_multilabeltoword(self):\n",
    "        mapping={'':'I','0':'D','0.0':'D','1':'R','1.0':'R'}\n",
    "        return {k:''.join([mapping[x] for x in list(v)]) for k,v in self.isolate2label_vec_mapping.items()}\n",
    "    \n",
    "    def drug_similarity(self, fileaddress):\n",
    "        multilabels=list(DA.get_multilabeltoword().values())\n",
    "        N=len(self.drug_names)\n",
    "        MI=np.zeros((N,N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                MI[i,j]=metrics.mutual_info_score([label[i] for label in list(multilabels)],[label[j] for label in list(multilabels)])\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xticklabels(['']+self.drug_names,rotation=45)\n",
    "        ax.set_yticklabels(['']+self.drug_names)\n",
    "        plt.imshow(MI, cmap='binary' )\n",
    "        plt.savefig(fileaddress)\n",
    "        return MI\n",
    "    \n",
    "    def resistance_frequency_analysis(self):\n",
    "        multilabels=list(self.get_multilabeltoword().values())\n",
    "        return FreqDist(multilabels)\n",
    "    \n",
    "    def return_most_k_frequent_classes(self, n):\n",
    "        most_common_labels=[x for x,y in self.resistance_frequency_analysis().most_common(n)]\n",
    "        joint_labels=self.get_multilabeltoword()\n",
    "        selected_isolates=[x for x in self.common_islt if joint_labels[x] in most_common_labels]\n",
    "        X,Y=self.makeXY_prediction(self.X, self.get_multilabeltoword(), selected_isolates)\n",
    "        return X, Y, selected_isolates\n",
    "\n",
    "    def return_ova_labeling(self):\n",
    "        joint_labels=self.get_multilabeltoword()\n",
    "        all_labels=list(set(joint_labels.values()))\n",
    "        X,Y=self.makeXY_prediction(self.X, self.get_multilabeltoword(), self.common_islt)\n",
    "        labeling_schemes=dict()\n",
    "        for label in all_labels:\n",
    "            labeling_schemes[label]=[1 if y==label else 0 for y in Y]\n",
    "        return X, labeling_schemes\n",
    "    \n",
    "    def classifier_tuning(self, name, classifier, parameters):\n",
    "        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "        params=dict()\n",
    "        for drug in self.drug_names:\n",
    "            print ('parameter tuning for drug ',drug)\n",
    "            Xpart, Y=self.makeXY_prediction(self.X, self.drug2labeledisolates_mapping[drug], self.common_islt)\n",
    "            CT=ClassifierTuning(Xpart, Y, classifier, parameters)\n",
    "            CT.find_best(name+'_'+drug)\n",
    "            params[drug]=CT.clf\n",
    "        return params\n",
    "    \n",
    "    def classifier_testing_RF(self, parameters_file_prefix):\n",
    "        results=dict()\n",
    "        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=1)\n",
    "        for drug in self.drug_names:\n",
    "            print (drug)\n",
    "            results[drug]=[]\n",
    "            Xpart, Y=self.makeXY_prediction(self.X, self.drug2labeledisolates_mapping[drug], self.common_islt)\n",
    "            RF = pickle.load(open('tuned_params/'+parameters_file_prefix+\"_\"+drug+\".pickle\", \"rb\"))\n",
    "            parameters=RF.best_params_\n",
    "            clf_random_forest=RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "            max_depth=None, max_features=parameters['max_features'], min_samples_split=parameters['min_samples_split'] , min_samples_leaf=1, n_estimators=parameters['n_estimators'], n_jobs=30)\n",
    "            scores = cross_val_score(clf_random_forest, Xpart, Y, cv=cv, scoring='precision')\n",
    "            print(\"precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            results[drug].append(('RF','precision',scores.mean(), scores.std()))\n",
    "\n",
    "            scores = cross_val_score(clf_random_forest,Xpart, Y, cv=cv, scoring='recall')\n",
    "            print(\"recall: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            results[drug].append(('RF','recall',scores.mean(), scores.std()))\n",
    "\n",
    "            scores = cross_val_score(clf_random_forest, Xpart, Y, cv=cv, scoring='f1')\n",
    "            print(\"f1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            results[drug].append(('RF','f1',scores.mean(), scores.std()))\n",
    "\n",
    "            #scores = cross_val_score(clf_random_forest, Xpart, Y, cv=cv, scoring='roc_auc')\n",
    "            #print(\"roc_auc: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "            #results[drug].append(('RF','roc_auc',scores.mean(), scores.std()))\n",
    "    def extract_joint_relevant_features(self):\n",
    "        X,labeling_scheme=self.return_ova_labeling()\n",
    "        feature_names = self.feature_labels\n",
    "        selector = SelectKBest(chi2,k='all')\n",
    "\n",
    "\n",
    "        frequents=DA.resistance_frequency_analysis().most_common(100)\n",
    "\n",
    "        for rank, (l,freq) in enumerate(frequents):\n",
    "            print (l)\n",
    "            L=labeling_scheme[l]\n",
    "            selector.fit_transform(X, L )\n",
    "            scores = {feature_names[i]: x for i, x in enumerate(list(selector.scores_))}\n",
    "            scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)[0:100]\n",
    "            f = codecs.open('extracted_features/joint_drug/'+str(rank+1)+'_'+l+'_'+str(freq)+'_chi2.txt','w')\n",
    "            f.write('\\t'.join(['feature', 'score', 'avg I', 'avg O'])+'\\n')\n",
    "            for w, score in scores:\n",
    "                feature_array=X[:,feature_names.index(w)]\n",
    "                pos=[feature_array[idx] for idx, x in enumerate(L) if x==1]\n",
    "                neg=[feature_array[idx] for idx, x in enumerate(L) if x==0]\n",
    "                f.write('\\t'.join([str(w), str(score), str(round(np.average(pos),2))+'(+/-)'+str(round(np.std(pos),2)), str(round(np.average(neg),2))+'(+/-)'+str(round(np.std(neg),2))])+'\\n')\n",
    "            f.close()\n",
    "    def extract_drug_specific_features(self):\n",
    "        feature_names = self.feature_labels\n",
    "        selector = SelectKBest(chi2,k='all')\n",
    "\n",
    "        for drug in self.drug_names:\n",
    "            print (drug)\n",
    "            Xpart, Y=self.makeXY_prediction(self.X, self.drug2labeledisolates_mapping[drug], self.common_islt)\n",
    "            selector.fit_transform(Xpart, Y )\n",
    "            scores = {feature_names[i]: x for i, x in enumerate(list(selector.scores_))}\n",
    "            scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)[0:100]\n",
    "            f = codecs.open('extracted_features/separate_drug/'+drug+'_chi2.txt','w')\n",
    "            f.write('\\t'.join(['feature', 'score', 'avg I', 'avg O'])+'\\n')\n",
    "            for w, score in scores:\n",
    "                feature_array=Xpart[:,feature_names.index(w)]\n",
    "                pos=[feature_array[idx] for idx, x in enumerate(Y) if x==1]\n",
    "                neg=[feature_array[idx] for idx, x in enumerate(Y) if x==0]\n",
    "                f.write('\\t'.join([str(w), str(score), str(round(np.average(pos),2))+'(+/-)'+str(round(np.std(pos),2)), str(round(np.average(neg),2))+'(+/-)'+str(round(np.std(neg),2))])+'\\n')\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data access created..\n"
     ]
    }
   ],
   "source": [
    "data_dir='/mounts/data/proj/asgari/github_data/data/pseudomonas/data/'\n",
    "labels_addr='MIC/v2/mic_bin_without_intermediate.txt'\n",
    "genexp_data_addr='new/gene_expression/rpg_log_transformed_426.txt'\n",
    "snps_data_addr='snp/v2/non-syn_SNPs_bin.txt'\n",
    "gen_pres_abs_data_addr='new/annot.txt'\n",
    "\n",
    "DA=ABRUtility(data_dir, labels_addr, genexp_data_addr, snps_data_addr, gen_pres_abs_data_addr)\n",
    "DA.load_all_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_random_forest=RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "            max_depth=None, max_features='auto', min_samples_leaf=1, n_estimators=500, n_jobs=30)\n",
    "\n",
    "param_grid = {\"n_estimators\": [100,200, 500, 1000],\n",
    "              \"criterion\": [\"gini\", \"entropy\"],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "             'min_samples_split':[2,5,10]}\n",
    "\n",
    "#RF_params=DA.classifier_tuning('random_forst',clf_random_forest,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_svm=svm.SVC(kernel='linear', C=1)\n",
    "\n",
    "param_grid = {'kernel':['linear', 'poly', 'rbf', 'sigmoid']\n",
    "             'C':list(range())}\n",
    "\n",
    "RF_params=DA.classifier_tuning('svm',clf_random_forest,param_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
