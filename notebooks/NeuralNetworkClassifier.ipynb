{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from data_access.data_access_utility import ABRDataAccess\n",
    "from classifier.classical_classifiers import SVM, RFClassifier, KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data access created..\n",
      "@@@/mounts/data/proj/asgari/dissertation/datasets/deepbio/pseudomonas/data_v3/snps_nonsyn_trimmed_feature_vect.npz\n",
      "@@@/mounts/data/proj/asgari/dissertation/datasets/deepbio/pseudomonas/data_v3/gpa_feature_vect.npz\n",
      "@@@/mounts/data/proj/asgari/dissertation/datasets/deepbio/pseudomonas/data_v3/genexp_norm01_feature_vect.npz\n"
     ]
    }
   ],
   "source": [
    "feature_list=['snps_nonsyn_trimmed','gpa','genexp_norm01']\n",
    "ABRAccess=ABRDataAccess('/mounts/data/proj/asgari/dissertation/datasets/deepbio/pseudomonas/data_v3/',feature_list)\n",
    "\n",
    "drug_idx=2\n",
    "drug_vect=['Ciprofloxacin', 'Tobramycin', 'Colistin', 'Ceftazidim', 'Meropenem']\n",
    "drug=drug_vect[drug_idx]\n",
    "X,Y,features, iso=ABRAccess.get_xy_prediction_mats(drug,mapping={'0': 0, '0.0': 0, '1': 1, '1.0': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utility.file_utility import FileUtility\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from gensim.models.wrappers import FastText\n",
    "import itertools\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "class DNNAMRClassifier(object):\n",
    "    \n",
    "    def __init__(self, X,Y, model_arch=[500]):\n",
    "        # rep. X\n",
    "        self.X=X\n",
    "        # encoding Y\n",
    "        self.Y=Y\n",
    "        self.C=len(set(Y))\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(Y)\n",
    "        self.encoded_Y = encoder.transform(Y)\n",
    "        self.onehot_y = np_utils.to_categorical(self.encoded_Y)\n",
    "        # model's arch\n",
    "        self.model_arch=model_arch\n",
    "    \n",
    "    def get_MLP_model(self):\n",
    "        # creating the model\n",
    "        model = Sequential()\n",
    "        for layer_idx, h_layer_size in enumerate(self.model_arch):\n",
    "            if layer_idx==0:\n",
    "                model.add(Dense(h_layer_size, input_dim=self.X.shape[1], activation='relu'))\n",
    "            else:\n",
    "                if h_layer_size < 1:\n",
    "                    model.add(Dropout(h_layer_size))\n",
    "                else:\n",
    "                    model.add(Dense(h_layer_size, activation='relu'))\n",
    "        model.add(Dense(self.C, activation='softmax'))        \n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def get_EMB_MLP_model(self):\n",
    "        # creating the model\n",
    "        model = Sequential()\n",
    "        embedding=np.load('../../datasets/processed_data/embedding/WV_16s.npz')['arr_0']\n",
    "        model.add(Dense(embedding.shape[1], input_dim=self.X.shape[1],  weights = [embedding, np.array([0]*1000)], trainable=False , activation='linear'))\n",
    "        for layer_idx, h_layer_size in enumerate(self.model_arch):\n",
    "            if h_layer_size < 1:\n",
    "                model.add(Dropout(h_layer_size))\n",
    "            else:\n",
    "                model.add(Dense(h_layer_size, activation='relu'))\n",
    "        model.add(Dense(self.C, activation='softmax'))        \n",
    "        # Compile model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self.model_arch=[1000]+self.model_arch\n",
    "        return model\n",
    "    \n",
    "    def get_pretrained_model(self, file_name, trainable):\n",
    "        pretrained_weights=FileUtility.load_obj(file_name)\n",
    "        \n",
    "        h_sizes=[float(x) for x in file_name.split('/')[-1].split('_')[3].split('-')]\n",
    "        model = Sequential()\n",
    "        for layer_idx, h_layer_size in enumerate(h_sizes):\n",
    "            if layer_idx==0:\n",
    "                model.add(Dense(int(h_layer_size), input_dim=self.X.shape[1], weights=pretrained_weights[0],  activation='relu', trainable=trainable))\n",
    "            else:\n",
    "                if h_layer_size < 1:\n",
    "                    model.add(Dropout(h_layer_size, weights=pretrained_weights[layer_idx], trainable=trainable))\n",
    "                else:\n",
    "                    model.add(Dense(int(h_layer_size), weights=pretrained_weights[layer_idx], activation='relu', trainable=trainable))\n",
    "        if self.model_arch:\n",
    "            for layer_idx, h_layer_size in enumerate(self.model_arch):\n",
    "                if h_layer_size < 1:\n",
    "                    model.add(Dropout(h_layer_size))\n",
    "                else:\n",
    "                    model.add(Dense(h_layer_size, activation='relu'))\n",
    "        model.add(Dense(self.C, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "        \n",
    "    \n",
    "    def cross_validation(self, result_filename, gpu_dev='2', n_fold=5, epochs=50, batch_size=100, model_strct='mlp', pretrained_model=False, trainable=False):\n",
    "\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_dev\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=n_fold, shuffle=True)\n",
    "\n",
    "        p_micro=[]\n",
    "        p_macro=[]\n",
    "        r_micro=[]\n",
    "        r_macro=[]\n",
    "        f1_micro=[]\n",
    "        f1_macro=[]\n",
    "\n",
    "        for train_index, valid_index in skf.split(self.X, self.Y):\n",
    "            print ('\\n Evaluation on a new fold is now get started ..')\n",
    "            X_train=self.X[train_index,:]\n",
    "            y_train=self.onehot_y[train_index,:]\n",
    "            y_class_train=self.encoded_Y[train_index]\n",
    "            X_valid=self.X[valid_index,:]\n",
    "            y_valid=self.onehot_y[valid_index,:]\n",
    "            y_class_valid=self.encoded_Y[valid_index]\n",
    "            \n",
    "            if pretrained_model:\n",
    "                model=self.get_pretrained_model(model_strct, trainable)\n",
    "            else:\n",
    "                if model_strct=='mlp':\n",
    "                    model=self.get_MLP_model()\n",
    "                if model_strct=='embmlp':\n",
    "                    model=self.get_EMB_MLP_model()\n",
    "            \n",
    "            # fitting\n",
    "            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,shuffle=True, validation_data=(X_valid, y_valid), verbose=0)\n",
    "            pred=model.predict_classes(X_valid)\n",
    "            # score-calculations\n",
    "            f1_micro.append(f1_score(pred, y_class_valid, average='micro'))\n",
    "            f1_macro.append(f1_score(pred, y_class_valid, average='macro'))\n",
    "            p_micro.append(precision_score(pred, y_class_valid, average='micro'))\n",
    "            p_macro.append(precision_score(pred, y_class_valid, average='macro'))\n",
    "            r_micro.append(recall_score(pred, y_class_valid, average='micro'))\n",
    "            r_macro.append(recall_score(pred, y_class_valid, average='macro'))\n",
    "\n",
    "        # mean values\n",
    "        f1mac=np.mean(f1_macro)\n",
    "        f1mic=np.mean(f1_micro)\n",
    "        prmac=np.mean(p_macro)\n",
    "        prmic=np.mean(p_micro)\n",
    "        remac=np.mean(r_macro)\n",
    "        remic=np.mean(r_micro)\n",
    "        # std values\n",
    "        sf1mac=np.std(f1_macro)\n",
    "        sf1mic=np.std(f1_micro)\n",
    "        sprmac=np.std(p_macro)\n",
    "        sprmic=np.std(p_micro)\n",
    "        sremac=np.std(r_macro)\n",
    "        sremic=np.std(r_micro)\n",
    "        # table\n",
    "        latex_line=' & '.join([str(np.round(x,2))+' $\\\\pm$ '+str(np.round(y,2)) for x,y in [[prmic, sprmic], [remic, sremic], [f1mic, sf1mic], [prmac, sprmac], [remac, sremac], [f1mac, sf1mac] ]])      \n",
    "        \n",
    "        print (latex_line)\n",
    "        \n",
    "        \n",
    "        \n",
    "        history_dict = history.history\n",
    "        loss_values = history_dict['loss']\n",
    "        val_loss_values = history_dict['val_loss']\n",
    "        epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "        if pretrained_model:\n",
    "            model_strct='pretrained'\n",
    "            #print (model.summary())\n",
    "        FileUtility.save_obj('_'.join([result_filename, model_strct,'-'.join([str(x) for x in self.model_arch]), str(np.round(f1mac,2))]), [latex_line, p_micro, r_micro, f1_micro, p_macro, r_macro, f1_macro, (loss_values, val_loss_values, epochs)])\n",
    "        \n",
    "        weights=[]\n",
    "        for x in model.layers:\n",
    "            weights.append(x.get_weights())\n",
    "        \n",
    "        FileUtility.save_obj('_'.join([result_filename, 'layers', model_strct,'-'.join([str(x) for x in self.model_arch]), str(np.round(f1mac,2))]), weights)\n",
    "        FileUtility.save_list('_'.join([result_filename, 'train_index', model_strct,'-'.join([str(x) for x in self.model_arch]), str(np.round(f1mac,2))]),train_index)\n",
    "        FileUtility.save_list('_'.join([result_filename, 'test_index', model_strct,'-'.join([str(x) for x in self.model_arch]), str(np.round(f1mac,2))]),valid_index)    \n",
    "\n",
    "    @staticmethod\n",
    "    def load_history(filename, fileout):\n",
    "        [latex_line, p_micro, r_micro, f1_micro, p_macro, r_macro, f1_macro, history]=FileUtility.load_obj(filename)\n",
    "        (loss_values, val_loss_values, epochs)=history\n",
    "        matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "        matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "        matplotlib.rcParams['mathtext.fontset'] = 'custom'\n",
    "        matplotlib.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
    "        matplotlib.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
    "        matplotlib.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
    "        matplotlib.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "        matplotlib.rcParams[\"axes.linewidth\"] = 0.6\n",
    "        plt.rc('text', usetex=True)\n",
    "        plt.plot(epochs, loss_values, 'ro', label='Loss for train set')\n",
    "        plt.plot(epochs, val_loss_values, 'b+', label='Loss for test set')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(loc=1, prop={'size': 8},ncol=1, edgecolor='black', facecolor='white', frameon=True)\n",
    "        plt.title('Loss with respect to the number of epochs for train and test sets')\n",
    "        plt.savefig(fileout+'.pdf')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def make_activation_function(file_name, X, last_layer=None):\n",
    "        pretrained_weights=FileUtility.load_obj(file_name)\n",
    "        if last_layer:\n",
    "            h_sizes=[float(x) for x in file_name.split('/')[-1].split('_')[3].split('-')]+[last_layer]\n",
    "        else:\n",
    "            h_sizes=[float(x) for x in file_name.split('/')[-1].split('_')[3].split('-')]\n",
    "        model = Sequential()\n",
    "        for layer_idx, h_layer_size in enumerate(h_sizes):\n",
    "            if layer_idx==0:\n",
    "                model.add(Dense(int(h_layer_size), input_dim=X.shape[1], weights=pretrained_weights[0],  activation='relu'))\n",
    "            else:\n",
    "                if h_layer_size < 1:\n",
    "                    model.add(Dropout(h_layer_size, weights=pretrained_weights[layer_idx]))\n",
    "                else:\n",
    "                    if layer_idx == len(h_sizes)-1 and last_layer:\n",
    "                        model.add(Dense(int(h_layer_size), weights=pretrained_weights[layer_idx], activation='softmax'))\n",
    "                    else:\n",
    "                        model.add(Dense(int(h_layer_size), weights=pretrained_weights[layer_idx], activation='relu'))\n",
    "        activations = model.predict(X)\n",
    "        np.savetxt(file_name.replace(file_name.split('/')[-1].split('_')[0],'activationlayer'),activations)\n",
    "        return activations\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def result_visualization(filename):\n",
    "        [latex_line, p_micro, r_micro, f1_micro, p_macro, r_macro, f1_macro, (loss_values, val_loss_values, epochs)]=FileUtility.load_obj(filename)\n",
    "        print(latex_line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation on a new fold is now get started ..\n",
      "128/128 [==============================] - 0s     \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "\n",
      " Evaluation on a new fold is now get started ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/student/asgari/.local/lib/python3.4/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/mounts/Users/student/asgari/.local/lib/python3.4/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for drug_idx in [0,1,2,3,4]:\n",
    "    drug_vect=['Ciprofloxacin', 'Tobramycin', 'Colistin', 'Ceftazidim', 'Meropenem']\n",
    "    drug=drug_vect[drug_idx]\n",
    "    X,Y,features, iso=ABRAccess.get_xy_prediction_mats(drug,mapping={'0': 0, '0.0': 0, '1': 1, '1.0': 1})\n",
    "    DNN=DNNAMRClassifier(X.toarray(),Y,model_arch=[1000,0.2,512])\n",
    "    DNN.cross_validation('../results/classification/dnn/classifier_'+drug, gpu_dev='2', n_fold=3, epochs=30, batch_size=3, model_strct='mlp')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DNNAMRClassifier' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4594c90c4c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DNNAMRClassifier' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "history=DNN.history\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAESCAYAAAAfXrn0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d47d9be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'custom'\n",
    "matplotlib.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
    "matplotlib.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
    "matplotlib.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
    "matplotlib.rcParams[\"axes.edgecolor\"] = \"black\"\n",
    "matplotlib.rcParams[\"axes.linewidth\"] = 0.6\n",
    "plt.rc('text', usetex=True)\n",
    "plt.plot(epochs, loss_values, 'ro', label='Loss for train set')\n",
    "plt.plot(epochs, val_loss_values, 'b+', label='Loss for test set')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=1, prop={'size': 8},ncol=1, edgecolor='black', facecolor='white', frameon=True)\n",
    "plt.title('Loss with respect to the number of epochs for train and test sets')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y_hat=DNN.model.predict_on_batch(X[1:6,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.01805095,\n",
       "        -0.00303337,  0.00430073],\n",
       "       [ 1.        ,  0.        ,  0.        , ..., -0.01002263,\n",
       "         0.01075347, -0.01498816],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.01681682,\n",
       "        -0.01322764,  0.02109493],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.01847024,\n",
       "        -0.01287623,  0.02317673],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.01630178,\n",
       "        -0.00991178,  0.02462367]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:6,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
